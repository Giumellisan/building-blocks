{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semesterarbeit Teil 2\n",
    "\n",
    "Bei diesem Teil der Semesterarbeit ist das Ziel eine eigene Python-Klasse zu erstellen. Die Klasse sollte in der Lage sein, die weit verbreiteten NLP- Ansätze der term frequency und inverse document frequency umzusetzen. Die txt-Dokumente befinden sich in der selben directory wie das Py-file und das Jupyternotebook. Bei einem anderen Speicherort, müsste man den Dateipfad ausschreiben. \n",
    "\n",
    "Nachfolgend die nötigen Definitionen der TF und IDF - Methoden:\n",
    "\n",
    "Die inverse document frequency : $idf(wort)$ ist der Logarithmus des Verhältnisses der Länge des Korpus (Anzahl Dictionaries) zur Anzahl derer Dokumente, in welchen das gesuchte Wort enthalten ist.\n",
    "\n",
    "$\\text{idf}(\\text{wort}) = \\log_{10}\\left(\\frac{\\text{len(korpus)}}{\\text{df   +1}}\\right)$\n",
    "\n",
    "df = Anzahl Dokumente aus dem Korpus welche Wort enthalten (1 Vorkommen genügt). Das Ziel der IDF ist es seltene Wörter in dem Datensatz hochzuskalieren. Ein Füllwort, welches oft vorkommt hat eine hohe TF ist aber nicht wirklich wichtig. Ein Wort, dass in allen Datensätzen vorkommt wird einen IDF von 0 bekommen, da $ log(N/N) = 0$ ergibt. Je seltener das Wort, desto grösser wird der Bruch und somit IDF.\n",
    "\n",
    "Für sehr grosse Texte müssen wir den logarithmus zur Basis 10 nehmen, damit der idf-score nicht durch die Decke schiesst. Für Texte in denen ein Wort nicht vorkommt wird DF 0 sein. Da wir nicht durch 0 dividieren können addieren wir 1 zum Nenner.\n",
    "\n",
    "\n",
    "Term Frequency misst die relative Häufigkeit eines Wortes innerhalb eines vorgegebenen Dokuments. Ein Wort welches oft vorkommt, erhält folgerichtig auch einen hohen Wert. Füllwörter und Konjunktionen werden vorderhand oft aus dem Text-string herausgerechnet, da diese immer sehr häufig vorkommen. Der Wertebereich liegt zwischen 0 und 1:\n",
    "\n",
    "$\\text{tf(wort, dokument)} = \\frac{\\text{Anzahl Vorkommen von wort im Dokument}}{\\text{Gesamtzahl aller Wörter im Dokument}}$\n",
    "\n",
    "\n",
    "Schlussendlich berechnet man den TF-IDF Score \n",
    " tf(wort, dokument) * idf(wort, korpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus :\n",
    "    \"\"\"\n",
    "    Die Klasse Corpus liest und analysiert Dokumente, welche wir mit der Hilf\n",
    "    klasse Document zuvor eingelesen haben. Zuerst wird aus dem txt.file die\n",
    "    Wörter herausgefiltert in einen Dictionary mit keys = wort und values = Anzahl Vorkommen des Wortes\n",
    "    im key. Die Methoden tf und idf beziehen sich dann auf den Dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self) : #initialise\n",
    "        self.documents = [] \n",
    "    \n",
    "             \n",
    "    def read_document(self, document_name, path) :\n",
    "        \n",
    "        text =[]#Hilfsvariablen brauchen Zuordnung\n",
    "        with open (path) as datei: #Hilfsvariablen brauchen Zuordnung\n",
    "            for line in datei:\n",
    "                text += line.split()\n",
    "        \n",
    "        result = {}\n",
    "        for w in text:\n",
    "            result[w] = text.count(w)\n",
    "                    \n",
    "        self.documents.append(Document(document_name, path, result))\n",
    "        #return result\n",
    "    \n",
    "    def list_documents_names(self) : \n",
    "         \n",
    "        result = []\n",
    "        for doc in self.documents: \n",
    "            result.append(doc.name)\n",
    "        return result\n",
    "    \n",
    "     \n",
    "    \n",
    "    def print_document(self, document) :\n",
    "        \"Ausdrucken des Inhalts des angegebenen Inhalts\" \n",
    "        dok = self.find_document(document)\n",
    "        if dok != None :\n",
    "            print(dok.inhalt)         \n",
    "\n",
    "    \n",
    "    def remove_document(self, document_name) :\n",
    "        \"Entfernt ein Dokument welches sich fälchlischerweise eingeschlichen hat\"\n",
    "        for doc in self.documents:\n",
    "            if document_name == doc.name :\n",
    "                self.documents.remove(doc)\n",
    "        \n",
    "    \n",
    "    def tf(self, term, document_name) :\n",
    "        \"Falls term in \"\n",
    "        \n",
    "        document = self.find_document(document_name)\n",
    "        \n",
    "        if term in document.word_count:\n",
    "            x = document.word_count[term]\n",
    "            return x / max(document.word_count.values())        \n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    \n",
    "        \n",
    "    def idf(self, term) :\n",
    "        \"mit dem Zähler zählen wir alle Dokumente in denen term vorkommt\"\n",
    "        \n",
    "        zaehler = 0\n",
    "        for doc in self.documents:\n",
    "            if term in doc.word_count:\n",
    "                zaehler += 1\n",
    "               \n",
    "        return math.log10 (len(corp.documents) /(zaehler + 1))\n",
    "        \n",
    "    def tf_idf(self, term, document_name) :\n",
    "        \n",
    "        return self.tf(term, document_name) * self.idf(term)\n",
    "    \n",
    "    def tf_idf_all(self, document_name):\n",
    "        \"Dies würde einen Dictionary mit den tf--idf scores aller Wörter generieren\"\n",
    "        result ={}\n",
    "        for term in self.find_document(document_name).word_count:\n",
    "            result[term] =  self.tf_idf(term, document_name)\n",
    "        return result\n",
    "            \n",
    "        \n",
    "    def find_document(self, document_name) : #Hilfsmethode\n",
    "        for doc in self.documents:\n",
    "            if ( doc.name == document_name):\n",
    "                return doc\n",
    "        print (\"Kein Dokument vorhanden\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = Corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document doc1 has count {'Helmut': 6, 'Kohl': 3, 'Insel': 3, 'Bruder': 3, 'Fritz': 1, 'Hilfe': 1},\n",
       " Document doc2 has count {'Nikosia': 1, 'Kohl': 3, 'Helmut': 5, 'Insel': 5, 'Bruder': 3, 'Fritz': 1, 'Muster': 1, 'Hilfe': 1},\n",
       " Document doc3 has count {'Helmut': 4, 'Kohl': 1, 'Insel': 5, 'Nikosia': 1, 'Bruder': 4, 'Fritz': 1},\n",
       " Document doc4 has count {'Helmut': 4, 'Kohl': 2, 'Insel': 8, 'Bruder': 5, 'Fritz': 2, 'Muster': 1},\n",
       " Document doc5 has count {'Insel': 3, 'Helmut': 3, 'Hilfe': 2, 'Bruder': 4, 'Fritz': 2, 'Muster': 1}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.read_document('doc1','input1.txt')\n",
    "corp.read_document('doc2','input2.txt')\n",
    "corp.read_document('doc3','input3.txt')\n",
    "corp.read_document('doc4','input4.txt')\n",
    "corp.read_document('doc5','input5.txt')\n",
    "corp.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document doc1 has count {'Helmut': 6, 'Kohl': 3, 'Insel': 3, 'Bruder': 3, 'Fritz': 1, 'Hilfe': 1},\n",
       " Document doc3 has count {'Helmut': 4, 'Kohl': 1, 'Insel': 5, 'Nikosia': 1, 'Bruder': 4, 'Fritz': 1},\n",
       " Document doc4 has count {'Helmut': 4, 'Kohl': 2, 'Insel': 8, 'Bruder': 5, 'Fritz': 2, 'Muster': 1},\n",
       " Document doc5 has count {'Insel': 3, 'Helmut': 3, 'Hilfe': 2, 'Bruder': 4, 'Fritz': 2, 'Muster': 1}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.remove_document('doc2')\n",
    "corp.documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Methode remove_document loopt durch die Dokumente welche sich in dem Klassenattribut self.dokumente befinden und macht einen Abgleich mit dem Namen einer Hilfsvariable für Dokumente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(corp.documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die built-in len() Funktion ist nützlich für die idf- Methode, da wir sie verwenden um den Zähler zu bestimmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doc1', 'doc3', 'doc4', 'doc5']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.list_documents_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese Methode populiert eine leere Liste mit den Ergebnissen einer for-Schleife welche durch alle Dokumente innerhalb des Klassenattributs Dokumente geht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-0efe20afac88>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-0efe20afac88>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    corp.\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "corp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document :\n",
    "    \"\"\"\n",
    "        Hilfsklasse für ein Dokument. \n",
    "        Eine Instanz dieser Klasse enthält\n",
    "        Name und Pfad des Dokuments, den Inhalt des Dokuments als String\n",
    "        und ein Dictionary mit Word-Counts.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, path, word_count) :\n",
    "        self.name = name\n",
    "        self.path = path\n",
    "        #self.content = content # Inhalts-text als string\n",
    "        self.word_count = word_count\n",
    "        \n",
    "    def __repr__(self) : #es zeigt eine vorschau des ausdrucks, wie soll das dokument beschrieben \n",
    "    #builtin-klassen haben diese Vorschau bereits- siehe z.B die Klasse LISTE!\n",
    "    \n",
    "        return \"Document {} has count {}\".format(self.name,self.word_count)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Der Dokumentname lautet:{} mit {} Wörtern\".format(self.name,self.word_count)\n",
    "    # gibt einen String aus- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f96f9d9bedfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'doc1' is not defined"
     ]
    }
   ],
   "source": [
    "print(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.tf('Nikosia','doc1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es macht Sinn, dass das Wort Nikosia eine tf von 0 in doc1 erhält, da es kein einzige Mal vorkommt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3010299956639812"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.idf('Nikosia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Funktion idf ergibt für das Wort Helmut einen Wert von -0.09691001. Dies deutet auf ein hohes Vorkommen des Wortes Helmut hin und tatsächlich kommt dieses Wort in allen Dokumenten vor. Wir behelfen uns der Hilfsmethode find_document um eine hilfsvariable Dokument mit dem Attribut word_count der Dokuments-klasse zu verknüpfen. Wir dividieren das Vorkommen von wort term durch das Maximum an Wörtern in dem Dokument. Wenn term nicht vorkomm geben wir 0 aus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.09691001300805639"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = math.log10(4/5)\n",
    "x #Nachweis das idf richtig rechnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp.tf_idf('Kohl','doc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kein Dokument vorhanden\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'word_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-485c1c18cee6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcorp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_idf_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'doc2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-91f952213804>\u001b[0m in \u001b[0;36mtf_idf_all\u001b[1;34m(self, document_name)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;34m\"Dies würde einen Dictionary mit den tf--idf scores aller Wörter generieren\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mterm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_document\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_idf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocument_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'word_count'"
     ]
    }
   ],
   "source": [
    "corp.tf_idf_all('doc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='corp.pkl'\n",
    "\n",
    "f = open(file_name,'wb')\n",
    "pickle.dump(corp,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
